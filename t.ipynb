{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MoZ3WW3IWiyF"
      },
      "outputs": [],
      "source": [
        "! pip install \"granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gfqKiyGQWiyI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
        "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
        "\n",
        "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback, count_parameters, get_datasets\n",
        "from tsfm_public.toolkit.get_model import get_model\n",
        "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
        "from tsfm_public.toolkit.visualization import plot_predictions\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "import warnings\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYPPzcXSXviI",
        "outputId": "aa54a2f4-7b53-4357-b7a2-75578dfe22cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['main.ipynb',\n",
              " 'AdditionalDataset',\n",
              " '.DS_Store',\n",
              " 'EDA&Processing',\n",
              " 'README.md',\n",
              " 'ttm_finetuned_models',\n",
              " '.gitignore',\n",
              " '.git',\n",
              " 'comodity-price-prediction-penyisihan-arkavidia-9',\n",
              " 't.ipynb']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8534TTFZWiyJ"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
        "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
        "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
        "# TTM_MODEL_PATH = \"ibm-research/ttm-research-r2\"\n",
        "\n",
        "# Context length, Or Length of the history.\n",
        "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
        "CONTEXT_LENGTH = 512\n",
        "\n",
        "# Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
        "PREDICTION_LENGTH = 96\n",
        "\n",
        "TARGET_DATASET = \"etth1\"\n",
        "dataset_path = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
        "\n",
        "\n",
        "# Results dir\n",
        "OUT_DIR = \"ttm_finetuned_models/\"\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J_BQ_oEJWiyK"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "TARGET_DATASET = \"datavidia\"\n",
        "dataset_path = \"./AdditionalDataset/training_dataset.csv\"\n",
        "test_dataset_path = \"./AdditionalDataset/testing_dataset.csv\"\n",
        "timestamp_column = \"Date\"\n",
        "id_columns = ['commodity', 'province']\n",
        "target_columns = ['price']\n",
        "split_config = {\n",
        "    \"train\": 0.8,\n",
        "    \"test\": 0.1\n",
        "}\n",
        "\n",
        "test_split_config = {\n",
        "    \"train\": 0.8,\n",
        "    \"test\": 0.1\n",
        "}\n",
        "\n",
        "# Understanding the split config -- slides\n",
        "\n",
        "feature_to_scale = ['GlobalOpen', 'GlobalHigh', 'GlobalVol.', 'GlobalPrice', 'CE_Close', 'CE_High', 'CE_Low', 'CE_Open']\n",
        "\n",
        "data = pd.read_csv(\n",
        "    dataset_path,\n",
        "    parse_dates=[timestamp_column],\n",
        ")\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    test_dataset_path,\n",
        "    parse_dates=[timestamp_column],\n",
        ")\n",
        "\n",
        "column_specifiers = {\n",
        "    \"timestamp_column\": timestamp_column,\n",
        "    \"id_columns\": id_columns,\n",
        "    \"target_columns\": target_columns,\n",
        "    \"control_columns\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4wUyeKGhazUu"
      },
      "outputs": [],
      "source": [
        "def process_dataset(df: pd.DataFrame):\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df['timestamp'] = df['Date'].astype(int)\n",
        "    df['timestamp'] = df['timestamp'].div(10**9)\n",
        "\n",
        "    df['province'] = label_encoder.fit_transform(df['province'])\n",
        "    df['commodity'] = label_encoder.fit_transform(df['commodity'])\n",
        "\n",
        "    df = df.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "    for col in feature_to_scale:\n",
        "        df[col] = scaler.fit_transform(df[[col]])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DUKaQPMkWiyL"
      },
      "outputs": [],
      "source": [
        "data = process_dataset(data)\n",
        "test_data = process_dataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "3C9B1WLwWiyL",
        "outputId": "56deffa1-a0f1-4ad0-b538-76090e02ca56"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>commodity</th>\n",
              "      <th>province</th>\n",
              "      <th>price</th>\n",
              "      <th>GlobalOpen</th>\n",
              "      <th>GlobalHigh</th>\n",
              "      <th>GlobalLow</th>\n",
              "      <th>GlobalVol.</th>\n",
              "      <th>GlobalChange %</th>\n",
              "      <th>GlobalPrice</th>\n",
              "      <th>CE_Close</th>\n",
              "      <th>CE_High</th>\n",
              "      <th>CE_Low</th>\n",
              "      <th>CE_Open</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28970.0</td>\n",
              "      <td>0.384917</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>3.5</td>\n",
              "      <td>-1.266524</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.316524</td>\n",
              "      <td>-1.932679</td>\n",
              "      <td>-1.862012</td>\n",
              "      <td>1.437183</td>\n",
              "      <td>-1.87819</td>\n",
              "      <td>1.640995e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27440.0</td>\n",
              "      <td>0.384917</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>3.5</td>\n",
              "      <td>-1.266524</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.316524</td>\n",
              "      <td>-1.932679</td>\n",
              "      <td>-1.862012</td>\n",
              "      <td>1.437183</td>\n",
              "      <td>-1.87819</td>\n",
              "      <td>1.640995e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>11030.0</td>\n",
              "      <td>0.384917</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>3.5</td>\n",
              "      <td>-1.266524</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.316524</td>\n",
              "      <td>-1.932679</td>\n",
              "      <td>-1.862012</td>\n",
              "      <td>1.437183</td>\n",
              "      <td>-1.87819</td>\n",
              "      <td>1.640995e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>12080.0</td>\n",
              "      <td>0.384917</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>3.5</td>\n",
              "      <td>-1.266524</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.316524</td>\n",
              "      <td>-1.932679</td>\n",
              "      <td>-1.862012</td>\n",
              "      <td>1.437183</td>\n",
              "      <td>-1.87819</td>\n",
              "      <td>1.640995e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-01-01</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>22360.0</td>\n",
              "      <td>0.384917</td>\n",
              "      <td>0.977176</td>\n",
              "      <td>3.5</td>\n",
              "      <td>-1.266524</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.316524</td>\n",
              "      <td>-1.932679</td>\n",
              "      <td>-1.862012</td>\n",
              "      <td>1.437183</td>\n",
              "      <td>-1.87819</td>\n",
              "      <td>1.640995e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  commodity  province    price  GlobalOpen  GlobalHigh  GlobalLow  \\\n",
              "0 2022-01-01          0         0  28970.0    0.384917    0.977176        3.5   \n",
              "1 2022-01-01          1         0  27440.0    0.384917    0.977176        3.5   \n",
              "2 2022-01-01          2         0  11030.0    0.384917    0.977176        3.5   \n",
              "3 2022-01-01          3         0  12080.0    0.384917    0.977176        3.5   \n",
              "4 2022-01-01          4         0  22360.0    0.384917    0.977176        3.5   \n",
              "\n",
              "   GlobalVol.  GlobalChange %  GlobalPrice  CE_Close   CE_High    CE_Low  \\\n",
              "0   -1.266524           -0.48     0.316524 -1.932679 -1.862012  1.437183   \n",
              "1   -1.266524           -0.48     0.316524 -1.932679 -1.862012  1.437183   \n",
              "2   -1.266524           -0.48     0.316524 -1.932679 -1.862012  1.437183   \n",
              "3   -1.266524           -0.48     0.316524 -1.932679 -1.862012  1.437183   \n",
              "4   -1.266524           -0.48     0.316524 -1.932679 -1.862012  1.437183   \n",
              "\n",
              "   CE_Open     timestamp  \n",
              "0 -1.87819  1.640995e+09  \n",
              "1 -1.87819  1.640995e+09  \n",
              "2 -1.87819  1.640995e+09  \n",
              "3 -1.87819  1.640995e+09  \n",
              "4 -1.87819  1.640995e+09  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaICSLVA5iU-",
        "outputId": "e49cc595-afaa-4215-a61d-96b7f1782975"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40664"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data['price'] = 0\n",
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XOKY1TpIWiyN"
      },
      "outputs": [],
      "source": [
        "def fewshot_finetune_eval(\n",
        "    dataset_name,\n",
        "    batch_size,\n",
        "    learning_rate=None,\n",
        "    context_length=512,\n",
        "    forecast_length=96,\n",
        "    fewshot_percent=5,\n",
        "    freeze_backbone=True,\n",
        "    num_epochs=50,\n",
        "    save_dir=OUT_DIR,\n",
        "    loss=\"mse\",\n",
        "    quantile=0.5,\n",
        ") -> Trainer:\n",
        "    out_dir = os.path.join(save_dir, dataset_name)\n",
        "\n",
        "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
        "\n",
        "    # Data prep: Get dataset\n",
        "\n",
        "    tsp = TimeSeriesPreprocessor(\n",
        "        **column_specifiers,\n",
        "        context_length=context_length,\n",
        "        prediction_length=forecast_length,\n",
        "        scaling=True,\n",
        "        encode_categorical=False,\n",
        "        scaler_type=\"standard\",\n",
        "    )\n",
        "\n",
        "    dset_train, dset_val, dset_test = get_datasets(\n",
        "        tsp, data, split_config, fewshot_fraction=fewshot_percent / 100, fewshot_location=\"first\"\n",
        "    )\n",
        "    \n",
        "    # change head dropout to 0.7 for ett datasets\n",
        "    if \"ett\" in dataset_name:\n",
        "        finetune_forecast_model = get_model(\n",
        "            TTM_MODEL_PATH,\n",
        "            context_length=context_length,\n",
        "            prediction_length=forecast_length,\n",
        "            freq_prefix_tuning=False,\n",
        "            freq=None,\n",
        "            prefer_l1_loss=False,\n",
        "            prefer_longer_context=True,\n",
        "            # Can also provide TTM Config args\n",
        "            head_dropout=0.7,\n",
        "            loss=loss,\n",
        "            quantile=quantile,\n",
        "        )\n",
        "    else:\n",
        "        finetune_forecast_model = get_model(\n",
        "            TTM_MODEL_PATH,\n",
        "            context_length=context_length,\n",
        "            prediction_length=forecast_length,\n",
        "            freq_prefix_tuning=False,\n",
        "            freq=None,\n",
        "            prefer_l1_loss=False,\n",
        "            prefer_longer_context=True,\n",
        "            # Can also provide TTM Config args\n",
        "            head_dropout=1,\n",
        "            loss=loss,\n",
        "            quantile=quantile,\n",
        "        )\n",
        "\n",
        "    if freeze_backbone:\n",
        "        print(\n",
        "            \"Number of params before freezing backbone\",\n",
        "            count_parameters(finetune_forecast_model),\n",
        "        )\n",
        "\n",
        "        # Freeze the backbone of the model\n",
        "        for param in finetune_forecast_model.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Count params\n",
        "        print(\n",
        "            \"Number of params after freezing the backbone\",\n",
        "            count_parameters(finetune_forecast_model),\n",
        "        )\n",
        "\n",
        "    # Find optimal learning rate\n",
        "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
        "    if learning_rate is None:\n",
        "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
        "            finetune_forecast_model,\n",
        "            dset_train,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
        "\n",
        "    print(f\"Using learning rate = {learning_rate}\")\n",
        "    finetune_forecast_args = TrainingArguments(\n",
        "        output_dir=os.path.join(out_dir, \"output\"),\n",
        "        overwrite_output_dir=True,\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_epochs,\n",
        "        do_eval=True,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        dataloader_num_workers=8,\n",
        "        report_to=\"none\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
        "        load_best_model_at_end=True,  # Load the best model when training ends\n",
        "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
        "        greater_is_better=False,  # For loss\n",
        "        seed=SEED,\n",
        "    )\n",
        "\n",
        "    # Create the early stopping callback\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
        "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
        "    )\n",
        "    tracking_callback = TrackingCallback()\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        learning_rate,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
        "    )\n",
        "\n",
        "    finetune_forecast_trainer = Trainer(\n",
        "        model=finetune_forecast_model,\n",
        "        args=finetune_forecast_args,\n",
        "        train_dataset=dset_train,\n",
        "        eval_dataset=dset_val,\n",
        "        callbacks=[early_stopping_callback, tracking_callback],\n",
        "        optimizers=(optimizer, scheduler),\n",
        "    )\n",
        "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
        "\n",
        "    # Fine tune\n",
        "    finetune_forecast_trainer.train()\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
        "\n",
        "    finetune_forecast_trainer.model.loss = \"mse\"  # fixing metric to mse for evaluation\n",
        "\n",
        "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
        "    print(fewshot_output)\n",
        "    print(\"+\" * 60)\n",
        "\n",
        "    # get predictions\n",
        "\n",
        "    predictions_dict = finetune_forecast_trainer.predict(dset_test)\n",
        "\n",
        "    predictions_np = predictions_dict.predictions[0]\n",
        "\n",
        "    print(predictions_np.shape)\n",
        "\n",
        "    # get backbone embeddings (if needed for further analysis)\n",
        "\n",
        "    backbone_embedding = predictions_dict.predictions[1]\n",
        "\n",
        "    print(backbone_embedding.shape)\n",
        "\n",
        "    # plot\n",
        "    plot_predictions(\n",
        "        model=finetune_forecast_trainer.model,\n",
        "        dset=dset_test,\n",
        "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
        "        plot_prefix=\"test_fewshot\",\n",
        "        channel=0,\n",
        "    )\n",
        "\n",
        "    return finetune_forecast_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "EIcICKN3WiyN",
        "outputId": "217ec0eb-692f-4956-eeb8-653f48a83a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------- Running few-shot 5% --------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:p-60564:t-8603959808:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
            "INFO:p-60564:t-8603959808:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = main.\n",
            "INFO:p-60564:t-8603959808:get_model.py:get_model:[TTM] context_length = 512, prediction_length = 96\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of params before freezing backbone 805280\n",
            "Number of params after freezing the backbone 289696\n",
            "Using learning rate = 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:p-63087:t-8600125952:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63326:t-8637399552:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63365:t-8610906624:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63441:t-8673755648:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63498:t-8642155008:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63527:t-8675996160:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63606:t-8607179264:config.py:<module>:PyTorch version 2.2.2 available.\n",
            "INFO:p-63651:t-8646746624:config.py:<module>:PyTorch version 2.2.2 available.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 00:01, Epoch 0.93/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fewshot_finetune_eval(\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     dataset_name=TARGET_DATASET,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     context_length=CONTEXT_LENGTH,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     num_epochs=20\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfewshot_finetune_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONTEXT_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDICTION_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 138\u001b[0m, in \u001b[0;36mfewshot_finetune_eval\u001b[0;34m(dataset_name, batch_size, learning_rate, context_length, forecast_length, fewshot_percent, freeze_backbone, num_epochs, save_dir, loss, quantile)\u001b[0m\n\u001b[1;32m    135\u001b[0m finetune_forecast_trainer\u001b[38;5;241m.\u001b[39mremove_callback(INTEGRATION_TO_CALLBACK[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodecarbon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Fine tune\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[43mfinetune_forecast_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest MSE after few-shot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfewshot_percent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/transformers/trainer.py:2480\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2479\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2480\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2482\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/transformers/trainer.py:5153\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5152\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5153\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5154\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5155\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/accelerate/data_loader.py:575\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 575\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/Python312/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# fewshot_finetune_eval(\n",
        "#     dataset_name=TARGET_DATASET,\n",
        "#     context_length=CONTEXT_LENGTH,\n",
        "#     forecast_length=PREDICTION_LENGTH,\n",
        "#     batch_size=32,\n",
        "#     fewshot_percent=30,\n",
        "#     learning_rate=0.001,\n",
        "#     num_epochs=20\n",
        "# )\n",
        "\n",
        "fewshot_finetune_eval(\n",
        "    dataset_name=TARGET_DATASET,\n",
        "    context_length=CONTEXT_LENGTH,\n",
        "    forecast_length=PREDICTION_LENGTH,\n",
        "    batch_size=32,\n",
        "    fewshot_percent=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_with_finetuned_model(\n",
        "    model,                  # Use existing model instead of loading from path\n",
        "    new_data,\n",
        "    column_specifiers,\n",
        "    context_length=512,\n",
        "    forecast_length=96,\n",
        "    batch_size=16,\n",
        "    dataset_name=\"new_dataset\",\n",
        "    save_dir=\"./predictions\",\n",
        "):\n",
        "    # Create output directory\n",
        "    out_dir = os.path.join(save_dir, dataset_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    tsp = TimeSeriesPreprocessor(\n",
        "        **column_specifiers,\n",
        "        context_length=context_length,\n",
        "        prediction_length=forecast_length,\n",
        "        scaling=True,\n",
        "        encode_categorical=False,\n",
        "        scaler_type=\"standard\",\n",
        "    )\n",
        "    \n",
        "    # Process the new data to create a test dataset\n",
        "    _, _, test_dataset = get_datasets(\n",
        "        tsp, new_data, {\"test\": 1.0}, fewshot_fraction=0, fewshot_location=\"first\"\n",
        "    )\n",
        "    \n",
        "    print(f'Test dataset length: {len(test_dataset)}')\n",
        "    \n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a trainer for prediction\n",
        "    inference_args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        dataloader_num_workers=4,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    \n",
        "    predictor = Trainer(\n",
        "        model=model,\n",
        "        args=inference_args,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "    \n",
        "    # Run prediction\n",
        "    predictions_dict = predictor.predict(test_dataset)\n",
        "    predictions_np = predictions_dict.predictions[0]\n",
        "    \n",
        "    print(f\"Predictions shape: {predictions_np.shape}\")\n",
        "    \n",
        "    # Get backbone embeddings if available\n",
        "    if len(predictions_dict.predictions) > 1:\n",
        "        backbone_embedding = predictions_dict.predictions[1]\n",
        "        print(f\"Backbone embeddings shape: {backbone_embedding.shape}\")\n",
        "    \n",
        "    # Visualize predictions\n",
        "    plot_predictions(\n",
        "        model=model,\n",
        "        dset=test_dataset,\n",
        "        plot_dir=out_dir,\n",
        "        plot_prefix=\"new_data_predictions\",\n",
        "        channel=0,\n",
        "    )\n",
        "    \n",
        "    return predictions_np, test_dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
