{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
    "\n",
    "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback, count_parameters, get_datasets\n",
    "from tsfm_public.toolkit.get_model import get_model\n",
    "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
    "# TTM_MODEL_PATH = \"ibm-research/ttm-research-r2\"\n",
    "\n",
    "# Context length, Or Length of the history.\n",
    "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "# Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
    "PREDICTION_LENGTH = 96\n",
    "\n",
    "TARGET_DATASET = \"etth1\"\n",
    "dataset_path = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv\"\n",
    "\n",
    "\n",
    "# Results dir\n",
    "OUT_DIR = \"ttm_finetuned_models/\"\n",
    "\n",
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "TARGET_DATASET = \"datavidia\"\n",
    "dataset_path = \"./AdditionalDataset/training_dataset.csv\"\n",
    "timestamp_column = \"Date\"\n",
    "id_columns = ['commodity', 'province']\n",
    "target_columns = ['price']\n",
    "split_config = {\n",
    "    \"train\": [0, 8640],\n",
    "    \"valid\": [8640, 11520],\n",
    "    \"test\": [\n",
    "        11520,\n",
    "        14400,\n",
    "    ],\n",
    "}\n",
    "# Understanding the split config -- slides\n",
    "\n",
    "data = pd.read_csv(\n",
    "    dataset_path,\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": timestamp_column,\n",
    "    \"id_columns\": id_columns,\n",
    "    \"target_columns\": target_columns,\n",
    "    \"control_columns\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['timestamp'] = data['Date'].astype(int)\n",
    "data['timestamp'] = data['timestamp'].div(10**9)\n",
    "\n",
    "data['province'] = label_encoder.fit_transform(data['province'])\n",
    "data['commodity'] = label_encoder.fit_transform(data['commodity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>commodity</th>\n",
       "      <th>province</th>\n",
       "      <th>price</th>\n",
       "      <th>GlobalOpen</th>\n",
       "      <th>GlobalHigh</th>\n",
       "      <th>GlobalLow</th>\n",
       "      <th>GlobalVol.</th>\n",
       "      <th>GlobalChange %</th>\n",
       "      <th>GlobalPrice</th>\n",
       "      <th>CE_Close</th>\n",
       "      <th>CE_High</th>\n",
       "      <th>CE_Low</th>\n",
       "      <th>CE_Open</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28970.0</td>\n",
       "      <td>205.9048</td>\n",
       "      <td>782.25</td>\n",
       "      <td>3.5</td>\n",
       "      <td>228640.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>202.7536</td>\n",
       "      <td>3554.002863</td>\n",
       "      <td>14301.299805</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>3561.752985</td>\n",
       "      <td>1.640995e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27440.0</td>\n",
       "      <td>205.9048</td>\n",
       "      <td>782.25</td>\n",
       "      <td>3.5</td>\n",
       "      <td>228640.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>202.7536</td>\n",
       "      <td>3554.002863</td>\n",
       "      <td>14301.299805</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>3561.752985</td>\n",
       "      <td>1.640995e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11030.0</td>\n",
       "      <td>205.9048</td>\n",
       "      <td>782.25</td>\n",
       "      <td>3.5</td>\n",
       "      <td>228640.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>202.7536</td>\n",
       "      <td>3554.002863</td>\n",
       "      <td>14301.299805</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>3561.752985</td>\n",
       "      <td>1.640995e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>205.9048</td>\n",
       "      <td>782.25</td>\n",
       "      <td>3.5</td>\n",
       "      <td>228640.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>202.7536</td>\n",
       "      <td>3554.002863</td>\n",
       "      <td>14301.299805</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>3561.752985</td>\n",
       "      <td>1.640995e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22360.0</td>\n",
       "      <td>205.9048</td>\n",
       "      <td>782.25</td>\n",
       "      <td>3.5</td>\n",
       "      <td>228640.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>202.7536</td>\n",
       "      <td>3554.002863</td>\n",
       "      <td>14301.299805</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>3561.752985</td>\n",
       "      <td>1.640995e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  commodity  province    price  GlobalOpen  GlobalHigh  GlobalLow  \\\n",
       "0 2022-01-01          0         0  28970.0    205.9048      782.25        3.5   \n",
       "1 2022-01-01          1         0  27440.0    205.9048      782.25        3.5   \n",
       "2 2022-01-01          2         0  11030.0    205.9048      782.25        3.5   \n",
       "3 2022-01-01          3         0  12080.0    205.9048      782.25        3.5   \n",
       "4 2022-01-01          4         0  22360.0    205.9048      782.25        3.5   \n",
       "\n",
       "   GlobalVol.  GlobalChange %  GlobalPrice     CE_Close       CE_High  \\\n",
       "0    228640.0           -0.48     202.7536  3554.002863  14301.299805   \n",
       "1    228640.0           -0.48     202.7536  3554.002863  14301.299805   \n",
       "2    228640.0           -0.48     202.7536  3554.002863  14301.299805   \n",
       "3    228640.0           -0.48     202.7536  3554.002863  14301.299805   \n",
       "4    228640.0           -0.48     202.7536  3554.002863  14301.299805   \n",
       "\n",
       "     CE_Low      CE_Open     timestamp  \n",
       "0  0.029976  3561.752985  1.640995e+09  \n",
       "1  0.029976  3561.752985  1.640995e+09  \n",
       "2  0.029976  3561.752985  1.640995e+09  \n",
       "3  0.029976  3561.752985  1.640995e+09  \n",
       "4  0.029976  3561.752985  1.640995e+09  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_finetune_eval(\n",
    "    dataset_name,\n",
    "    batch_size,\n",
    "    learning_rate=None,\n",
    "    context_length=512,\n",
    "    forecast_length=96,\n",
    "    fewshot_percent=5,\n",
    "    freeze_backbone=True,\n",
    "    num_epochs=50,\n",
    "    save_dir=OUT_DIR,\n",
    "    loss=\"mse\",\n",
    "    quantile=0.5,\n",
    "):\n",
    "    out_dir = os.path.join(save_dir, dataset_name)\n",
    "\n",
    "    print(\"-\" * 20, f\"Running few-shot {fewshot_percent}%\", \"-\" * 20)\n",
    "\n",
    "    # Data prep: Get dataset\n",
    "\n",
    "    tsp = TimeSeriesPreprocessor(\n",
    "        **column_specifiers,\n",
    "        context_length=context_length,\n",
    "        prediction_length=forecast_length,\n",
    "        scaling=True,\n",
    "        encode_categorical=False,\n",
    "        scaler_type=\"standard\",\n",
    "    )\n",
    "\n",
    "    dset_train, dset_val, dset_test = get_datasets(\n",
    "        tsp, data, fewshot_fraction=fewshot_percent / 100, fewshot_location=\"first\"\n",
    "    )\n",
    "\n",
    "    # change head dropout to 0.7 for ett datasets\n",
    "    if \"ett\" in dataset_name:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=context_length,\n",
    "            prediction_length=forecast_length,\n",
    "            freq_prefix_tuning=False,\n",
    "            freq=None,\n",
    "            prefer_l1_loss=False,\n",
    "            prefer_longer_context=True,\n",
    "            # Can also provide TTM Config args\n",
    "            head_dropout=0.7,\n",
    "            loss=loss,\n",
    "            quantile=quantile,\n",
    "        )\n",
    "    else:\n",
    "        finetune_forecast_model = get_model(\n",
    "            TTM_MODEL_PATH,\n",
    "            context_length=context_length,\n",
    "            prediction_length=forecast_length,\n",
    "            freq_prefix_tuning=False,\n",
    "            freq=None,\n",
    "            prefer_l1_loss=False,\n",
    "            prefer_longer_context=True,\n",
    "            # Can also provide TTM Config args\n",
    "            loss=loss,\n",
    "            quantile=quantile,\n",
    "        )\n",
    "\n",
    "    if freeze_backbone:\n",
    "        print(\n",
    "            \"Number of params before freezing backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "        # Freeze the backbone of the model\n",
    "        for param in finetune_forecast_model.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Count params\n",
    "        print(\n",
    "            \"Number of params after freezing the backbone\",\n",
    "            count_parameters(finetune_forecast_model),\n",
    "        )\n",
    "\n",
    "    # Find optimal learning rate\n",
    "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
    "    if learning_rate is None:\n",
    "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
    "            finetune_forecast_model,\n",
    "            dset_train,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
    "\n",
    "    print(f\"Using learning rate = {learning_rate}\")\n",
    "    finetune_forecast_args = TrainingArguments(\n",
    "        output_dir=os.path.join(out_dir, \"output\"),\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        dataloader_num_workers=8,\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
    "        load_best_model_at_end=True,  # Load the best model when training ends\n",
    "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "        greater_is_better=False,  # For loss\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    # Create the early stopping callback\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
    "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
    "    )\n",
    "    tracking_callback = TrackingCallback()\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
    "    )\n",
    "\n",
    "    finetune_forecast_trainer = Trainer(\n",
    "        model=finetune_forecast_model,\n",
    "        args=finetune_forecast_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "        callbacks=[early_stopping_callback, tracking_callback],\n",
    "        optimizers=(optimizer, scheduler),\n",
    "    )\n",
    "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
    "\n",
    "    # Fine tune\n",
    "    finetune_forecast_trainer.train()\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
    "\n",
    "    finetune_forecast_trainer.model.loss = \"mse\"  # fixing metric to mse for evaluation\n",
    "\n",
    "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
    "    print(fewshot_output)\n",
    "    print(\"+\" * 60)\n",
    "\n",
    "    # get predictions\n",
    "\n",
    "    predictions_dict = finetune_forecast_trainer.predict(dset_test)\n",
    "\n",
    "    predictions_np = predictions_dict.predictions[0]\n",
    "\n",
    "    print(predictions_np.shape)\n",
    "\n",
    "    # get backbone embeddings (if needed for further analysis)\n",
    "\n",
    "    backbone_embedding = predictions_dict.predictions[1]\n",
    "\n",
    "    print(backbone_embedding.shape)\n",
    "\n",
    "    # plot\n",
    "    plot_predictions(\n",
    "        model=finetune_forecast_trainer.model,\n",
    "        dset=dset_test,\n",
    "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
    "        plot_prefix=\"test_fewshot\",\n",
    "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
    "        channel=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Running few-shot 5% --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000024ED2221C60>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1576, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "INFO:p-25812:t-24924:get_model.py:get_model:Loading model from: ibm-granite/granite-timeseries-ttm-r2\n",
      "INFO:p-25812:t-24924:get_model.py:get_model:Model loaded successfully from ibm-granite/granite-timeseries-ttm-r2, revision = main.\n",
      "INFO:p-25812:t-24924:get_model.py:get_model:[TTM] context_length = 512, prediction_length = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params before freezing backbone 805280\n",
      "Number of params after freezing the backbone 289696\n",
      "Using learning rate = 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/350 00:00 < 00:13, 24.88 it/s, Epoch 0.86/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfewshot_finetune_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_DATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONTEXT_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDICTION_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 137\u001b[0m, in \u001b[0;36mfewshot_finetune_eval\u001b[1;34m(dataset_name, batch_size, learning_rate, context_length, forecast_length, fewshot_percent, freeze_backbone, num_epochs, save_dir, loss, quantile)\u001b[0m\n\u001b[0;32m    134\u001b[0m finetune_forecast_trainer\u001b[38;5;241m.\u001b[39mremove_callback(INTEGRATION_TO_CALLBACK[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodecarbon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Fine tune\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[43mfinetune_forecast_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest MSE after few-shot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfewshot_percent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% fine-tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2639\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2639\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2643\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3085\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[0;32m   3083\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3085\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3086\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3039\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3039\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3042\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4105\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4104\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4106\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4115\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4289\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4286\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4288\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 4289\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   4290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Update the observed num examples\u001b[39;49;00m\n\u001b[0;32m   4291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfind_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:547\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_epoch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration)\n\u001b[1;32m--> 547\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\LM23-2\\AppData\\Local\\anaconda3\\envs\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fewshot_finetune_eval(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_length=PREDICTION_LENGTH,\n",
    "    batch_size=64,\n",
    "    fewshot_percent=5,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
